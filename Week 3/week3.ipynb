{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 3\n",
    "## Efficiently finding optimal policies in MABs\n",
    "\n",
    "In this assignment, we will work with Multi Armed Bandit environments, and try to find the best policies using different strategies to minimize the total regret.\n",
    "\n",
    "The aim of this exercise is to code agents capable of understanding the underlying probability distributions of the environment and finding the most optimal actions as early as possible.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary stuff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# if you want to use envs from Gym, import it\n",
    "# import gym, gym_bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple **2-armed Bernoulli** bandit.\n",
    "\n",
    "If you want a cleaner code, you can implement Bandits using `class` in Python.\n",
    "\n",
    "We have included sample code for this in `bandits.py` which you can take/import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Prob. Dist. for Bernoulli Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the underlying probability distribution\n",
    "\n",
    "probs = np.random.random(2)\n",
    "# for action 0, reward is +1 with chance probs[0] and 0 with chance 1-probs[0], and similarly for action 1\n",
    "print(probs) # however, probs is not known to the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAB Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a function which takes an action and returns a reward\n",
    "def mab_2_env(action):\n",
    "    gen = np.random.random()\n",
    "    # for bernoulli bandits, the reward is 1 if the random number is less than the probability of success, else 0\n",
    "    return gen < probs[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy returns a random action\n",
    "pure_exploration = lambda Q : np.random.randint(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exponentially Decaying Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentially_decaying_epsilon_greedy(Q, epsilon, episode, gamma):\n",
    "    epsilon = epsilon * (gamma ** episode)\n",
    "\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q, temperature):\n",
    "\n",
    "    # probs_list = []\n",
    "    list_of_probs = []\n",
    "    sum_of_exp = 0\n",
    "\n",
    "    for i in range(len(Q)):\n",
    "        exp = np.exp(Q[i]/temperature)\n",
    "        list_of_probs.append(exp)\n",
    "        sum_of_exp += exp\n",
    "\n",
    "    for j in range(len(list_of_probs)):\n",
    "        list_of_probs[j] /= sum_of_exp\n",
    "\n",
    "    return np.random.choice(len(Q), 1, list_of_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(Q, N, c):\n",
    "\n",
    "    # sum(N) yields the number of episodes happened till now\n",
    "    s = sum(N)\n",
    "\n",
    "    U = dict()\n",
    "\n",
    "    if s < len(Q):\n",
    "        return Q[int(s)]\n",
    "    else:\n",
    "        for action in range(len(Q)):\n",
    "            U[action] = Q[action] + c*np.sqrt(np.log(s)/N[action])\n",
    "    \n",
    "        return np.argmax(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thompson sampling strategy\n",
    "\n",
    "def thompson_sampling(Q, N, alpha, beta):\n",
    "    samples = np.random.normal(loc=Q, scale = alpha/(np.sqrt(N) + beta))\n",
    "    return np.argmax(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Regret to compare Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, the regret $\\mathcal{T}$ is given by,\n",
    "\n",
    "$$\\mathcal{T}=\\sum  _{e=1} ^{E} \\mathbb{E} \\left[ v_* - q_* \\left( A_e \\right) \\right]$$\n",
    "\n",
    "We can only calculate it when we have the $v_*$ and $q_*$ functions known beforehand. Since we are making the MDPs from scratch, that's not an issue for us right now.\n",
    "\n",
    "But remember, in real-life problems, these functions are not known. Hence we must be aware of multiple policy finding strategies and try the one which gives best results fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total regret\n",
    "\n",
    "# true expected reward of the optimal action = reward[argmax(probs)] * probs[argmax(probs)]\n",
    "\n",
    "def regret(reward, action, probs):\n",
    "    # reward is the actual reward (here, +1), actions are 0 to n - 1, probs[i] = prob(getting reward on action i)\n",
    "    optimal_expected_reward = np.max(probs) * reward\n",
    "    expected_reward_on_action = probs[action] * reward\n",
    "    return optimal_expected_reward - expected_reward_on_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy function takes in the environment function, number of actions, and a selector function\n",
    "# it also takes in the number of episodes to run the strategy for (higher episodes = more accurate Q values)\n",
    "\n",
    "def test_strategy(env, n_actions, strategy_name, n_times = 10, n_episodes = 1000, \n",
    "                  eg_epsilon = 0.01, ed_epsilon = 1, ed_gamma = 0.1, \n",
    "                  sft_temp = 1000, ucb_c = 2, thmp_alpha = 1, \n",
    "                  thmp_beta = 0, reward = 1):\n",
    "    \n",
    "    avg_regret = 0\n",
    "    \n",
    "    for _ in range(n_times):\n",
    "        # return if it found the optimal action, and also the total regret\n",
    "        \n",
    "        # env is the mdp function, selector is the function which selects an action given the Q dict\n",
    "        \n",
    "        # initialize Q and N to 0s\n",
    "        Q = np.zeros(n_actions)\n",
    "        N = np.zeros(n_actions)\n",
    "    \n",
    "        total_regret = 0\n",
    "    \n",
    "        # loop for n_episodes\n",
    "        for e in tqdm(range(n_episodes)):\n",
    "            \n",
    "            # selector function takes in current Q and returns an action\n",
    "            # modify the selector function according to the strategy\n",
    "            if strategy_name == \"pure exploration\":\n",
    "                action = pure_exploration(Q)\n",
    "            elif strategy_name == \"epsilon greedy\":\n",
    "                action = epsilon_greedy(Q, eg_epsilon)\n",
    "            elif strategy_name == \"exp decaying e greedy\":\n",
    "                action = exponentially_decaying_epsilon_greedy(Q, ed_epsilon, sum(N), ed_gamma)\n",
    "            elif strategy_name == \"softmax\":\n",
    "                action = softmax(Q, sft_temp)\n",
    "            elif strategy_name == \"ucb\":\n",
    "                action = ucb(Q, N, ucb_c)\n",
    "            elif strategy_name == \"thompson\":\n",
    "                action = thompson_sampling(Q, sum(N), thmp_alpha, thmp_beta)\n",
    "    \n",
    "            cur_regret = regret(reward, action, probs)\n",
    "            total_regret += cur_regret\n",
    "    \n",
    "            # get the reward from the environment\n",
    "            reward = env(action)\n",
    "    \n",
    "            # update N and Q\n",
    "            N[action] += 1\n",
    "            Q[action] += (reward - Q[action])/N[action]\n",
    "\n",
    "        avg_regret += total_regret/n_times\n",
    "    \n",
    "        # return the best action\n",
    "    return np.argmax(Q), avg_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"pure exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"epsilon greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"exp decaying e greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"ucb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, \"thompson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns the optimal action. Let's check if that's indeed true.\n",
    "\n",
    "We can do that by revealing the actual `probs` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 0 (done)\n",
    "\n",
    "Implement the calculation of the total regret $\\mathcal{T}$ for your strategy.\n",
    "\n",
    "To do this, you will need to store the rewards obtained each episode. Modify the `strategy` function accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 1 (done)\n",
    "\n",
    "Now, let's implement some other selection strategies and compare their regret with the simple exploration strategy.\n",
    "\n",
    "Note that some of these strategies involve hyperparameter(s) which need to be manually adjusted. You have to play around with the values and see which one gives you best results.\n",
    "\n",
    "This is known as \"hyperparameter tuning\" and is quite commonly done while working with complex models (including neural networks). Personally, you should try out some natural values (including the ones given in the book) along with some extreme values where it is easy to manually verify the correctness of your strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 2 (done)\n",
    "\n",
    "Run each strategy for 2-armed bandit environment and compare the total regrets.\n",
    "\n",
    "You can also try plotting the regret vs episode graph and check if it matches the expected result from Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 3\n",
    "\n",
    "The 2-armed bandits might be too simple for us to actually see substantial difference in the regret of these strategies. \n",
    "\n",
    "Let's now create a more complicated bandit environment and replicate our results on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a 10-armed Gaussian bandit. \n",
    "\n",
    "As required, it will have possible actions and each action will generate a reward sampled from a Gaussian distribution.\n",
    "\n",
    "Hence, each \"arm\" will have a randomly generated $\\mu$ and $\\sigma$, and the rewards will be generated with probabilities following the $\\mathcal{N}(\\mu, \\sigma^2)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 arm gaussian bandit\n",
    "\n",
    "# generate the means for each arm\n",
    "means = []\n",
    "\n",
    "# generate the variance for each arm\n",
    "variances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a again function which takes an action and returns a reward\n",
    "\n",
    "def mab_10_env(action):\n",
    "\n",
    "    # for gaussian bandits, the reward is generated from a normal distribution\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 4\n",
    "\n",
    "Test the different strategies on the 10-armed gaussian bandit and verify your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
