{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 2\n",
    "## Finding best policies in simple MDPs\n",
    "\n",
    "Great work making the MDPs in Week 1!\n",
    "\n",
    "In this assignment, we'll use the simplest RL techniques - Policy and Value iteration to find the best policies (which maximize the discounted total reward) in our MDPs from last week.\n",
    "\n",
    "Feel free to use your own MDPs, or import them from the OpenAI Gym library.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall the equation to find the value function of agent's states under a policy $\\pi$ -\n",
    "$$v_{\\pi}(s) = \\sum _{a} \\pi(a|s) ~ \\left( ~ \\sum _{s', r} ~ p(s', r | s, a) ~ \\left[r + \\gamma v_{\\pi}(s') \\right] ~ \\right)$$\n",
    "\n",
    "We can observe that the value function $v_{\\pi}$ has a lot of circular dependencies on different states. \n",
    "\n",
    "To solve such equations, one of the ways is to iteratively calculate the RHS and replace the LHS by it until the $v_{\\pi}(s)$ values start to converge. \n",
    "\n",
    "The point of convergence makes all the equations simultaneously true and hence is the required solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the value functions for some policies in the MDPs we created last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 0 - Bandit Walk\n",
    "\n",
    "Again, we consider the BW environment on Page 39.\n",
    "\n",
    "Let's consider what seems to be the most natural policy - always go Right.\n",
    "\n",
    "This environment is so simple, that we can simply calculate the value functions by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by convention for the terminal states, \n",
    "$$v_{\\pi}(0) = v_{\\pi}(2) = 0$$\n",
    "\n",
    "Now, \n",
    "$$v_{\\pi}(1) = 1 + \\gamma \\cdot v_{\\pi}(2) = 1$$\n",
    "\n",
    "Note both the summations just have one term due to the deterministic nature of the environment and the policy (check which summation was corresponding to which stochastic variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1 - Slippery Walk\n",
    "\n",
    "Let's now try to solve the SWF environment from Page 67 for the naturally adversarial policy - always go Left.\n",
    "\n",
    "Since we have 5 coupled equations for states 1-5 with 5 unknown variables, we'll use Python to bruteforce the solution.\n",
    "\n",
    "To align with Grokking, let us consider an unusual $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 is to import stuff\n",
    "\n",
    "import gym, gym_walk\n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MDP from gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, True),\n",
      "         (0.16666666666666666, 0, 0.0, True)],\n",
      "     1: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, True),\n",
      "         (0.16666666666666666, 0, 0.0, True)]},\n",
      " 1: {0: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.16666666666666666, 2, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.16666666666666666, 0, 0.0, True)]},\n",
      " 2: {0: [(0.5000000000000001, 1, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.16666666666666666, 3, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.16666666666666666, 1, 0.0, False)]},\n",
      " 3: {0: [(0.5000000000000001, 2, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.16666666666666666, 4, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 4, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.16666666666666666, 2, 0.0, False)]},\n",
      " 4: {0: [(0.5000000000000001, 3, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.16666666666666666, 5, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 5, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.16666666666666666, 3, 0.0, False)]},\n",
      " 5: {0: [(0.5000000000000001, 4, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, False),\n",
      "         (0.16666666666666666, 6, 1.0, True)],\n",
      "     1: [(0.5000000000000001, 6, 1.0, True),\n",
      "         (0.3333333333333333, 5, 0.0, False),\n",
      "         (0.16666666666666666, 4, 0.0, False)]},\n",
      " 6: {0: [(0.5000000000000001, 6, 0.0, True),\n",
      "         (0.3333333333333333, 6, 0.0, True),\n",
      "         (0.16666666666666666, 6, 0.0, True)],\n",
      "     1: [(0.5000000000000001, 6, 0.0, True),\n",
      "         (0.3333333333333333, 6, 0.0, True),\n",
      "         (0.16666666666666666, 6, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1 is to get the MDP\n",
    "\n",
    "env = gym.make('SlipperyWalkFive-v0')\n",
    "swf_mdp = env.P\n",
    "pprint.pprint(swf_mdp)\n",
    "\n",
    "# Note that in this mdp in gym, action \"Left\" is \"0\" and \"Right\" is \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n"
     ]
    }
   ],
   "source": [
    "# Step 2 is to write the policy\n",
    "# policy recommends action for each state (either deterministically or stochastically)\n",
    "\n",
    "pi = {\n",
    "    0 : 0,\n",
    "    1 : 0,\n",
    "    2 : 0,\n",
    "    3 : 0,\n",
    "    4 : 0,\n",
    "    5 : 0,\n",
    "    6 : 0\n",
    "}\n",
    "\n",
    "# Or you can initialize action for each state randomly\n",
    "\n",
    "pprint.pprint(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 is computing the value function for this environment and policy\n",
    "\n",
    "val = dict()\n",
    "# We could start with a random value function (after ensuring that values for all terminal states are 0)\n",
    "# Or, instead of doing this you can simply intialize the value function to 0 for all states \n",
    "for state in swf_mdp:\n",
    "    val[state] = 0\n",
    "\n",
    "pprint.pprint(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get new value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_value_fn(val, mdp, pi, gamma = 1.0):\n",
    "    # val is old value function\n",
    "    # mdp is mdp\n",
    "    # pi contains the action for each state\n",
    "    # gamma is discount\n",
    "    # new_val is new value function after 1 iteration over old value function\n",
    "    new_val = dict()\n",
    "    \n",
    "    for state in mdp:\n",
    "        action = pi[state] # action to be taken now given by deterministic policy\n",
    "        probabilities = mdp[state][action] # list of tuples\n",
    "        # for 5 and action 0 this is \n",
    "        \"\"\"\n",
    "        [(0.5000000000000001, 4, 0.0, False),\n",
    "         (0.3333333333333333, 5, 0.0, False),\n",
    "         (0.16666666666666666, 6, 1.0, True)]\n",
    "        \"\"\"\n",
    "        new_value_for_state = 0\n",
    "        \n",
    "        for (prob, next_state, reward, isTerminal) in probabilities:\n",
    "            new_value_for_state += prob * (reward + gamma * val[next_state] * (not isTerminal)) # in case value function\n",
    "            # is not set to 0 in all terminal states\n",
    "\n",
    "        new_val[state] = new_value_for_state\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.16666666666666666, 6: 0.0}\n",
      "{0: 0.0,\n",
      " 1: 0.0,\n",
      " 2: 0.0,\n",
      " 3: 0.0,\n",
      " 4: 0.027777777777777776,\n",
      " 5: 0.2222222222222222,\n",
      " 6: 0.0}\n",
      "{0: 0.0,\n",
      " 1: 0.0,\n",
      " 2: 0.0,\n",
      " 3: 0.004629629629629629,\n",
      " 4: 0.046296296296296294,\n",
      " 5: 0.25462962962962965,\n",
      " 6: 0.0}\n"
     ]
    }
   ],
   "source": [
    "# some trials to ensure function is correct (matches book)\n",
    "new_value_fn1 = get_new_value_fn(val, swf_mdp, pi, gamma = 1.0) \n",
    "pprint.pprint(new_value_fn1)\n",
    "new_value_fn2 = get_new_value_fn(new_value_fn1, swf_mdp, pi, gamma = 1.0)\n",
    "pprint.pprint(new_value_fn2)\n",
    "new_value_fn3 = get_new_value_fn(new_value_fn2, swf_mdp, pi, gamma = 1.0)\n",
    "pprint.pprint(new_value_fn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine when value functions begin to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this helper function has been written for policy evaluation to determine when \n",
    "# value functions have begun to converge\n",
    "\n",
    "def low_difference(value_fn1, value_fn2, epsilon = 1e-10):\n",
    "    # both must have same keys\n",
    "    similar = True\n",
    "    for i in value_fn1:\n",
    "        if abs(value_fn1[i] - value_fn2[i]) > epsilon:\n",
    "            similar = False\n",
    "    return similar # every element must be within epsilon of element of other value fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to above functions to get the final value function\n",
    "\n",
    "def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):\n",
    "    count = 0\n",
    "    while True:\n",
    "        val_new = get_new_value_fn(val, mdp, pi, gamma)\n",
    "        if low_difference(val_new, val, epsilon):\n",
    "            return val, count + 1\n",
    "        else:\n",
    "            count += 1\n",
    "            val = val_new\n",
    "            \n",
    "# Iteratively caluculate the value function until the difference between the new and old value function \n",
    "# is less than epsilon; also return the number of iterations it took to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0: 0.0,\n",
      "  1: 0.0027472526825522074,\n",
      "  2: 0.010989010794909371,\n",
      "  3: 0.03571428532608248,\n",
      "  4: 0.10989010930780505,\n",
      "  5: 0.3324175818352776,\n",
      "  6: 0.0},\n",
      " 104)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(policy_evaluation(val, swf_mdp, pi, gamma = 1)) # for swf, take gamma = 1 (as in book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine action with max expected return for state in q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for policy improvement, we need action in q function for which\n",
    "# expected return after following that action  and thereafter\n",
    "# following policy pi is maximum\n",
    "\n",
    "# this helper determines key with max value in dict (in q[state])\n",
    "\n",
    "def argmax(dict1):\n",
    "    items = list(dict1.items())\n",
    "    max_key = items[0][0]\n",
    "    max_val = items[0][1]\n",
    "    for (key, val) in items:\n",
    "        if val > max_val:\n",
    "            max_key = key\n",
    "            max_val = val\n",
    "\n",
    "    return max_key\n",
    "\n",
    "# print(argmax(policy_evaluation(val, swf_mdp, pi)[0])) # trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform policy improvement using the policy and the value function and return a new policy, \n",
    "# the action value function (q function) should be a nested dictionary\n",
    "\n",
    "def policy_improvement(val, mdp, gamma=1.0):\n",
    "    new_pi = dict()\n",
    "    q = dict()\n",
    "    # q must be something like\n",
    "    # {0: {0: val, 1: val}, 1: {0: val, 1: val},...}\n",
    "\n",
    "    for state in mdp:\n",
    "        q[state] = dict() # initialization, each value will be a dictionary\n",
    "        for action in mdp[state]:\n",
    "            q[state][action] = 0\n",
    "            # prob_tuples = mdp[state][action]\n",
    "            for (prob, next_state, reward, isTerminal) in mdp[state][action]:\n",
    "                q[state][action] += prob * (reward + gamma * val[next_state] * (not isTerminal))\n",
    "            \n",
    "    # after q has been made, \n",
    "    for state in q:\n",
    "        new_pi[state] = argmax(q[state]) # q[state] is a dict\n",
    "        \n",
    "    return new_pi, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(policy_improvement(val, swf_mdp, gamma = 1.0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above functions to get the optimal policy and optimal value function \n",
    "# and return the total number of iterations it took to converge.\n",
    "\n",
    "def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):\n",
    "    \n",
    "    optimal_pi = {s: 0 for s in mdp}\n",
    "    optimal_val = {s: 0 for s in mdp} # both pi and val are all zeroes initially\n",
    "    # as always, it is possible to create a random policy and value function to start with\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        new_pi = policy_improvement(optimal_val, mdp, gamma)[0]\n",
    "        if new_pi == optimal_pi: # convergence when policy cannot be optimized further\n",
    "            return optimal_pi, optimal_val, count + 1\n",
    "        else:\n",
    "            count += 1\n",
    "            optimal_pi = new_pi\n",
    "            optimal_val = policy_evaluation(optimal_val, mdp, optimal_pi, gamma = gamma)[0] # epsilon could be required here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 0},\n",
      " {0: 0.0,\n",
      "  1: 0.6675824170597608,\n",
      "  2: 0.8901098895872336,\n",
      "  3: 0.9642857139372767,\n",
      "  4: 0.9890109888367704,\n",
      "  5: 0.9972527471946744,\n",
      "  6: 0.0},\n",
      " 3)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(policy_iteration(swf_mdp, gamma = 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform value iteration, note that the value function is a dictionary and not a list,\n",
    "# also return the number of iterations it took to converge\n",
    "def value_iteration(mdp, gamma=1.0, epsilon=1e-10):\n",
    "    val = {s: 0 for s in mdp}\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        q = dict()\n",
    "\n",
    "        for state in mdp:\n",
    "            q[state] = dict()\n",
    "            for action in mdp[state]:\n",
    "                q[state][action] = 0\n",
    "                for (prob, next_state, reward, isTerminal) in mdp[state][action]:\n",
    "                    q[state][action] += prob * (reward + gamma * val[next_state] * (not isTerminal))\n",
    "\n",
    "        new_val = {state: max(q[state].values()) for state in mdp}\n",
    "                    \n",
    "        if low_difference(val, new_val, epsilon):\n",
    "            break\n",
    "\n",
    "        val = new_val.copy()\n",
    "        count += 1\n",
    "\n",
    "    pi = {s: 0 for s in mdp}\n",
    "    for state in mdp:\n",
    "        pi[state] = argmax(q[state])\n",
    "        \n",
    "    return val, pi, count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0: 0.0,\n",
      "  1: 0.6675824169918751,\n",
      "  2: 0.8901098895193477,\n",
      "  3: 0.9642857138920196,\n",
      "  4: 0.9890109888141417,\n",
      "  5: 0.9972527471871315,\n",
      "  6: 0.0},\n",
      " {0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 0},\n",
      " 122)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(value_iteration(swf_mdp, gamma = 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment 2 - Frozen Lake\n",
    "\n",
    "Repeat the above steps for the frozen lake environment. Don't create new functions , use the old functions.\n",
    "\n",
    "You can also write a function `test_policy()` to test your policy after training to find the number of times you reached the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)]},\n",
      " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)]},\n",
      " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)]},\n",
      " 5: {0: [(1.0, 5, 0, True)],\n",
      "     1: [(1.0, 5, 0, True)],\n",
      "     2: [(1.0, 5, 0, True)],\n",
      "     3: [(1.0, 5, 0, True)]},\n",
      " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)]},\n",
      " 7: {0: [(1.0, 7, 0, True)],\n",
      "     1: [(1.0, 7, 0, True)],\n",
      "     2: [(1.0, 7, 0, True)],\n",
      "     3: [(1.0, 7, 0, True)]},\n",
      " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     3: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)]},\n",
      " 11: {0: [(1.0, 11, 0, True)],\n",
      "      1: [(1.0, 11, 0, True)],\n",
      "      2: [(1.0, 11, 0, True)],\n",
      "      3: [(1.0, 11, 0, True)]},\n",
      " 12: {0: [(1.0, 12, 0, True)],\n",
      "      1: [(1.0, 12, 0, True)],\n",
      "      2: [(1.0, 12, 0, True)],\n",
      "      3: [(1.0, 12, 0, True)]},\n",
      " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      2: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True)]},\n",
      " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False)]},\n",
      " 15: {0: [(1.0, 15, 0, True)],\n",
      "      1: [(1.0, 15, 0, True)],\n",
      "      2: [(1.0, 15, 0, True)],\n",
      "      3: [(1.0, 15, 0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make('FrozenLake-v1')\n",
    "mdp2 = env2.P\n",
    "pprint.pprint(mdp2)\n",
    "\n",
    "# ACTIONS ARE ASSIGNED NUMBERS AS FOLLOWS: (to match fl env in open ai gym)\n",
    "# \"Left\" -> 0\n",
    "# \"Down\" -> 1\n",
    "# \"Right\" -> 2\n",
    "# \"Up\" -> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0,\n",
      " 1: 3,\n",
      " 2: 3,\n",
      " 3: 3,\n",
      " 4: 0,\n",
      " 5: 0,\n",
      " 6: 0,\n",
      " 7: 0,\n",
      " 8: 3,\n",
      " 9: 1,\n",
      " 10: 0,\n",
      " 11: 0,\n",
      " 12: 0,\n",
      " 13: 2,\n",
      " 14: 1,\n",
      " 15: 0}\n",
      "{0: 0.542025930296267,\n",
      " 1: 0.4988031849539974,\n",
      " 2: 0.4706956878752193,\n",
      " 3: 0.45685169676602944,\n",
      " 4: 0.5584509586627534,\n",
      " 5: 0.0,\n",
      " 6: 0.3583480707644934,\n",
      " 7: 0.0,\n",
      " 8: 0.5917987435152554,\n",
      " 9: 0.6430798237640515,\n",
      " 10: 0.6152075569912918,\n",
      " 11: 0.0,\n",
      " 12: 0.0,\n",
      " 13: 0.7417204382759435,\n",
      " 14: 0.8628374297788133,\n",
      " 15: 0.0}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "pi1, val1, count1 = policy_iteration(mdp2, gamma = 0.99)\n",
    "pprint.pprint(pi1)\n",
    "pprint.pprint(val1)\n",
    "pprint.pprint(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5420259303047927,\n",
      " 1: 0.49880318496538073,\n",
      " 2: 0.4706956878886318,\n",
      " 3: 0.45685169678049486,\n",
      " 4: 0.5584509586706584,\n",
      " 5: 0.0,\n",
      " 6: 0.35834807077058933,\n",
      " 7: 0.0,\n",
      " 8: 0.5917987435219645,\n",
      " 9: 0.6430798237690762,\n",
      " 10: 0.6152075569957234,\n",
      " 11: 0.0,\n",
      " 12: 0.0,\n",
      " 13: 0.7417204382795115,\n",
      " 14: 0.8628374297806647,\n",
      " 15: 0.0}\n",
      "{0: 0,\n",
      " 1: 3,\n",
      " 2: 3,\n",
      " 3: 3,\n",
      " 4: 0,\n",
      " 5: 0,\n",
      " 6: 0,\n",
      " 7: 0,\n",
      " 8: 3,\n",
      " 9: 1,\n",
      " 10: 0,\n",
      " 11: 0,\n",
      " 12: 0,\n",
      " 13: 2,\n",
      " 14: 1,\n",
      " 15: 0}\n",
      "571\n"
     ]
    }
   ],
   "source": [
    "pi2, val2, count2 = value_iteration(mdp2, gamma = 0.99)\n",
    "pprint.pprint(pi2)\n",
    "pprint.pprint(val2)\n",
    "pprint.pprint(count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing an optimal policy for frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIONS ARE ASSIGNED NUMBERS AS FOLLOWS: (to match fl env in open ai gym)\n",
    "# \"Left\" -> 0\n",
    "# \"Down\" -> 1\n",
    "# \"Right\" -> 2\n",
    "# \"Up\" -> 3\n",
    "\n",
    "terminal_states = [5, 7, 11, 12, 15]\n",
    "\n",
    "additions_dict = { # dictionary of actions with their corresponding additions to the current state\n",
    "    0: -1,\n",
    "    1: 4,\n",
    "    2: 1,\n",
    "    3: -4\n",
    "}\n",
    "actions_orthogonal_dict = { # for each key, value is set of actions orthogonal to it\n",
    "    0: (1, 3),\n",
    "    1: (0, 2),\n",
    "    2: (1, 3),\n",
    "    3: (0, 2),\n",
    "}\n",
    "\n",
    "def is_terminal(n):\n",
    "    return n in terminal_states\n",
    "\n",
    "def reward(n):\n",
    "    return float(n == 15) # (to match fl env in open ai gym)\n",
    "\n",
    "def add_change_to_state(cur_state, addition):\n",
    "    if abs(addition) == 4: # up or down, check only if new state remains inside lake\n",
    "        if cur_state + addition in range(16):\n",
    "            return cur_state + addition\n",
    "        else:\n",
    "            return cur_state\n",
    "            \n",
    "    elif abs(addition) == 1: # right or left, check only if new state is in same row as original state\n",
    "        if (cur_state // 4) == ((cur_state + addition) // 4):\n",
    "            return cur_state + addition\n",
    "        else:\n",
    "            return cur_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(pi, env, goalstate = 15):\n",
    "    # Complete this function to test the policy\n",
    "    successes = 0\n",
    "    failures = 0\n",
    "    \n",
    "    for i in range(10000):\n",
    "        current_state = 0\n",
    "        while True:\n",
    "            chance = random.random()\n",
    "            move_to_make = pi[current_state]\n",
    "            if chance < 1/3:\n",
    "                current_state = add_change_to_state(current_state, additions_dict[move_to_make])\n",
    "            elif chance < 2/3:\n",
    "                current_state = add_change_to_state(current_state, additions_dict[actions_orthogonal_dict[move_to_make][0]])\n",
    "            else:\n",
    "                current_state = add_change_to_state(current_state, additions_dict[actions_orthogonal_dict[move_to_make][1]])\n",
    "\n",
    "            if current_state == goalstate:\n",
    "                successes += 1\n",
    "                break\n",
    "                \n",
    "            elif current_state in terminal_states:\n",
    "                failures += 1\n",
    "                break                   \n",
    "                \n",
    "    return f\"{successes/100} % times it reached end state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.84 % times it reached end state\n",
      "83.24 % times it reached end state\n",
      "82.02 % times it reached end state\n",
      "81.67 % times it reached end state\n",
      "82.54 % times it reached end state\n",
      "81.53 % times it reached end state\n",
      "82.4 % times it reached end state\n",
      "82.41 % times it reached end state\n",
      "81.97 % times it reached end state\n",
      "81.9 % times it reached end state\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(test_policy(pi1, mdp2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
